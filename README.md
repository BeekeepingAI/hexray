# üî¨ HexRay

*HexRay is your scalpel, microscope, and headlamp for AI‚Äîtrace every decision as it forms and reveal its inner mysteries.*

---

## üöÄ What is HexRay?

HexRay is a low-level debugger for transformer models, purpose-built to illuminate the inner workings of AI ‚Äî token by token, layer by layer. Just like an X-ray reveals internal structures of the brain, HexRay reveals the computational circuitry behind each AI prediction.

Built on top of [TransformerLens](https://github.com/neelnanda-io/TransformerLens), HexRay empowers mechanistic interpretability (MI) ‚Äî the art of reverse engineering what algorithms a model has learned by analyzing weights, activations, and attention patterns. HexRay extends this with:

- üîç Logit debugging ‚Äî trace how specific logits emerge and which neurons or attention heads contributed most.
- üß† Chain-of-Thought attribution ‚Äî follow how reasoning unfolds across time steps and internal components.
- ü™ì Neuron and head introspection ‚Äî pinpoint influential subcomponents behind each decision.
- üß¨ Activation tracing ‚Äî monitor MLP and attention activity at every token and every layer.
- üß∞ Red team‚Äìready utilities ‚Äî test model robustness, adversarial triggers, and hidden circuits.

Whether you're reverse engineering AI, probing safety risks in frontier models, or unraveling the inner workings of large language models, HexRay equips you with a scalpel, microscope, neuroscope, and headlamp ‚Äî precision tools to illuminate, dissect, and understand the black box of AI with confidence.

---

## ‚ú® Features

- Token-by-token residual stream tracing ‚Äî inspect the evolution of hidden states at every layer and position.
- Logit debugging ‚Äî analyze which neurons, heads, and paths contributed most to a model‚Äôs final prediction.
- Chain-of-Thought (CoT) attribution ‚Äî trace logical reasoning step-by-step through attention and MLP layers.
- Top-k component attribution ‚Äî identify the most influential attention heads and MLP layers for each token.
- Layer-wise activation logging ‚Äî visualize and record intermediate activations for any prompt.
- CLI interface ‚Äî simple command-line interface for selecting models, prompts, and debugging modes.
- TransformerLens integration ‚Äî leverages robust hooks and interpretability primitives under the hood.
- Modular architecture ‚Äî designed for extensibility, including upcoming support for fuzzing, visualization, and adversarial tracing.
- Debugging modes ‚Äî toggle --cot-debug, --logit-debug, and --top-k-attribution to tailor your inspection workflow.
- Supports multiple model scales with easy swapping via CLI.

---

## üîß Basic Usage

```bash
python hexray.py --prompt "Why do bees buzz?" --top-k-attribution 10  
```

---

## üü© Output

```bash
Loading model gpt2-small...
Loaded pretrained model gpt2-small into HookedTransformer
Loaded pretrained model gpt2-small into HookedTransformer

Token: "
" (index -1)
Top Contributors to Final Logit:
- MLP  L9 : +46.16 (24.3%)
- MLP  L11: +35.61 (18.8%)
- MLP  L10: +30.14 (15.9%)
- MLP  L8 : +19.81 (10.4%)
- Attn L11: +14.21 (7.5%)
- MLP  L0 : +10.53 (5.5%)
- Attn L10: +9.93 (5.2%)
- Attn L0 : +9.62 (5.1%)
- Attn L7 : +7.67 (4.0%)
- MLP  L2 : +6.20 (3.3%)
```

# üîß Chain of Though Debugging

```bash
python hexray.py --prompt "If John has 3 apples..." --cot-debug --top-k-attribution 10
```

---

## üü© Output

```bash
Loading model gpt2-small...
Loaded pretrained model gpt2-small into HookedTransformer
Loaded pretrained model gpt2-small into HookedTransformer

üß† Chain of Thought Attribution Trace (Console):

Step 1: If John has 3 apples...
MLP  L10 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  23.2%
MLP  L8  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              12.6%
MLP  L0  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               11.5%
Attn L11 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               11.3%
Attn L0  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  9.7%
MLP  L7  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   9.0%
MLP  L11 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    7.9%
MLP  L6  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      6.3%
Attn L8  ‚ñà‚ñà‚ñà‚ñà‚ñà                       4.7%
Attn L9  ‚ñà‚ñà‚ñà                         3.7%
```

---

## Logit Debugging
```bash 
python hexray.py --model gpt2-xl --prompt "Tell me about Œ°hŒøenŒπx... Then if Ôº∞ÔΩÖÔΩêÔΩìÔΩâ is... the Œ°hŒøenŒπx must be..." --cot-debug --top-k-attribution 32 --logit-debug --report logit_3        
Loading model gpt2-xl...
config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 689/689 [00:00<00:00, 442kB/s]
model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.43G/6.43G [01:18<00:00, 82.1MB/s]
generation_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:00<00:00, 1.25MB/s]
tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26.0/26.0 [00:00<00:00, 318kB/s]
vocab.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.04M/1.04M [00:00<00:00, 5.73MB/s]
merges.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 456k/456k [00:00<00:00, 22.2MB/s]
tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.36M/1.36M [00:00<00:00, 21.8MB/s]
Loaded pretrained model gpt2-xl into HookedTransformer
Loaded pretrained model gpt2-xl into HookedTransformer
[‚Ä¢] Running Chain of Thought Debugger

üß† Chain of Thought Attribution Trace (Console):

Step 1: Tell me about Œ°hŒøenŒπx...
MLP  L44 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   7.0%
MLP  L42 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    7.0%
MLP  L45 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     6.6%
MLP  L41 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      6.4%
MLP  L39 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          5.2%
MLP  L43 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             4.4%
MLP  L36 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              4.2%
MLP  L37 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              4.0%
MLP  L40 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               3.8%
MLP  L38 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               3.8%
MLP  L33 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 3.2%
MLP  L46 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  3.0%
MLP  L34 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  2.9%
MLP  L29 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  2.9%
MLP  L35 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  2.8%
Attn L44 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   2.7%
Attn L42 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   2.6%
MLP  L32 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   2.5%
Attn L43 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    2.3%
Attn L46 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     2.2%
Attn L33 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      1.9%
MLP  L25 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      1.9%
Attn L39 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      1.9%
Attn L40 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      1.9%
MLP  L30 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      1.9%
Attn L36 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      1.9%
MLP  L28 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      1.8%
Attn L45 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      1.8%
MLP  L23 ‚ñà‚ñà‚ñà‚ñà‚ñà                       1.6%
MLP  L27 ‚ñà‚ñà‚ñà‚ñà                        1.3%
MLP  L0  ‚ñà‚ñà‚ñà‚ñà                        1.3%
Attn L37 ‚ñà‚ñà‚ñà‚ñà                        1.3%

Step 2: Then if Ôº∞ÔΩÖÔΩêÔΩìÔΩâ is... the Œ°hŒøenŒπx must be...
MLP  L44 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   9.5%
MLP  L43 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      8.4%
MLP  L42 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        7.9%
MLP  L45 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         7.6%
MLP  L47 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         7.3%
MLP  L46 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           6.6%
MLP  L41 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  4.0%
MLP  L39 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    3.4%
Attn L45 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    3.1%
Attn L42 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     3.0%
Attn L44 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     3.0%
Attn L43 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     2.8%
Attn L39 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      2.4%
Attn L37 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      2.3%
MLP  L40 ‚ñà‚ñà‚ñà‚ñà‚ñà                       2.3%
MLP  L38 ‚ñà‚ñà‚ñà‚ñà‚ñà                       2.3%
MLP  L34 ‚ñà‚ñà‚ñà‚ñà‚ñà                       2.0%
Attn L40 ‚ñà‚ñà‚ñà‚ñà‚ñà                       2.0%
MLP  L29 ‚ñà‚ñà‚ñà‚ñà                        1.9%
Attn L25 ‚ñà‚ñà‚ñà‚ñà                        1.8%
MLP  L35 ‚ñà‚ñà‚ñà‚ñà                        1.7%
MLP  L36 ‚ñà‚ñà‚ñà‚ñà                        1.6%
Attn L46 ‚ñà‚ñà‚ñà                         1.5%
Attn L41 ‚ñà‚ñà‚ñà                         1.5%
Attn L33 ‚ñà‚ñà‚ñà                         1.4%
MLP  L30 ‚ñà‚ñà‚ñà                         1.3%
Attn L34 ‚ñà‚ñà‚ñà                         1.3%
MLP  L23 ‚ñà‚ñà‚ñà                         1.3%
MLP  L37 ‚ñà‚ñà‚ñà                         1.2%
MLP  L25 ‚ñà‚ñà‚ñà                         1.2%
Attn L47 ‚ñà‚ñà‚ñà                         1.2%
Attn L35 ‚ñà‚ñà‚ñà                         1.1%
[‚Ä¢] Running Logit Debugger
[debug] captured: ['blocks.0.hook_attn_out', 'blocks.1.hook_attn_out', 'blocks.2.hook_attn_out', 'blocks.3.hook_attn_out', 'blocks.4.hook_attn_out', 'blocks.5.hook_attn_out', 'blocks.6.hook_attn_out', 'blocks.7.hook_attn_out', 'blocks.8.hook_attn_out', 'blocks.9.hook_attn_out', 'blocks.10.hook_attn_out', 'blocks.11.hook_attn_out', 'blocks.12.hook_attn_out', 'blocks.13.hook_attn_out', 'blocks.14.hook_attn_out', 'blocks.15.hook_attn_out', 'blocks.16.hook_attn_out', 'blocks.17.hook_attn_out', 'blocks.18.hook_attn_out', 'blocks.19.hook_attn_out', 'blocks.20.hook_attn_out', 'blocks.21.hook_attn_out', 'blocks.22.hook_attn_out', 'blocks.23.hook_attn_out', 'blocks.24.hook_attn_out', 'blocks.25.hook_attn_out', 'blocks.26.hook_attn_out', 'blocks.27.hook_attn_out', 'blocks.28.hook_attn_out', 'blocks.29.hook_attn_out', 'blocks.30.hook_attn_out', 'blocks.31.hook_attn_out', 'blocks.32.hook_attn_out', 'blocks.33.hook_attn_out', 'blocks.34.hook_attn_out', 'blocks.35.hook_attn_out', 'blocks.36.hook_attn_out', 'blocks.37.hook_attn_out', 'blocks.38.hook_attn_out', 'blocks.39.hook_attn_out', 'blocks.40.hook_attn_out', 'blocks.41.hook_attn_out', 'blocks.42.hook_attn_out', 'blocks.43.hook_attn_out', 'blocks.44.hook_attn_out', 'blocks.45.hook_attn_out', 'blocks.46.hook_attn_out', 'blocks.47.hook_attn_out']
[‚úì] Logit attribution plot saved to: logit_3/logit_attribution.png
```
---

## üìú License

MIT License ¬© 2025 Jonathan Jaquez
